# -*- coding: utf-8 -*-
"""Scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GVRQS_lPPvG4PGDtL9XREuSXiEv3eQit
"""

!pip install requests-html

'''
Please note that
1. You should change the cookies as it shows on your site
'''


import csv
import re, time
from requests_html import HTMLSession

session = HTMLSession()

class Demo(object):

    # Input keywords and time range
    q = input('Enter keyword: ')
    start_t = input('Enter start date, e.g., 2023-01-01: ')
    end_t = input('Enter end date, e.g., 2023-01-03: ')

    # Constructing the URL for Weibo search
    url = f'https://s.weibo.com/weibo?q={q}&typeall=1&suball=1&timescope=custom:{start_t}:{end_t}&Refer=g&page='

    # Setting cookie and headers to simulate browser requests
    cookie = 'SINAGLOBAL=2917303300462.415.1669034022036; UOR=m.weibo.cn,s.weibo.com,login.sina.com.cn; ULV=1688557237030:4:1:1:7281991153293.161.1688557237026:1678706160161; SCF=AmePAaumAaknp_ZlhZHC_DwG7X4JFZZG08GmBHtZBcoypHo9T4QMgczMz_jnAkye9P0kgUc1VxXmu2tzOnk5rAc.; SUB=_2AkMSFA8Rf8NxqwJRmP4QyGrhaIp0zAzEieKkSP7KJRMxHRl-yT9kqmE_tRB6OZQh_irnKM0dPbmc0tMT4lhkSbM8x0bU; SUBP=0033WrSXqPxfM72-Ws9jqgMF55529P9D9WFAVf1.CC-8hKhmI-CV_.NF; WBPSESS=kErNolfXeoisUDB3d9TFH208hFzZpzfyg7aOfWwxpShtlKtZZfdClBB-lZfMVcUsCyt7HB0PZfGyRi6_Z2kWjzkHtQehXfNNAkvvKKe-vilv2bHDIBgG_cCwX_2O9m9Ap2mj8bQnXRQvYbTuLtUsOr0DFVt9vi0xB9Ua1GDlNX0=; XSRF-TOKEN=t6HEwdo-6aHQgLuTBu9hUBlk'
    headers = {
        # These headers mimic common browser request headers
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-encoding': 'gzip, deflate',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'cookie': cookie,
        'referer': 'https://s.weibo.com/weibo?q=%E7%8B%82%E9%A3%99&typeall=1&suball=1&timescope=custom:2022-11-01:2023-02-01&Refer=g&page=2',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
    }

    # URL for fetching detailed Weibo data
    data_url = 'https://weibo.com/ajax/statuses/show?id={}'

    # Store already crawled Weibo IDs
    code_all = []

    # Create and open a CSV file for storing data
    w = open(f'{q}.csv', 'a', encoding='utf-8', newline='')
    w = csv.writer(w)
    # Writing the header of the CSV file
    w.writerow(['mid', 'Content', 'Post Location', 'Post Time', 'Likes', 'Reposts', 'Comments'])

    def start(self):
        for i in range(1, 51):  # Loop through the first 50 pages
            new_url = self.url + str(i)
            res = session.get(new_url, headers=self.headers).text  # Send request and get response

            # Use regex to find all Weibo links
            more_urls = re.findall(r'//weibo.com/[0-9]{1,}/.*?[?]refer_flag', res)
            for code in more_urls:
                code = code.split('/')[-1][:-len('?refer_flag')]
                if code not in self.code_all:  # If this ID has not been crawled yet
                    self.code_all.append(code)
                    print(code)
                    time.sleep(2)  # Pause to avoid being blocked

                    # Fetch detailed information of the Weibo
                    data_all = session.get(self.data_url.format(code), headers=self.headers).json()
                    mid = data_all['id']
                    text = data_all['text_raw'].replace('\n', '').replace('\u200b', '')
                    try:
                        place = data_all['region_name']
                    except:
                        place = 'none'
                    t = data_all['created_at']
                    attitudes_count = data_all['attitudes_count']
                    reposts_count = data_all['reposts_count']
                    comments_count = data_all['comments_count']

                    # Write data into CSV file
                    self.w.writerow([mid, text, place, t, attitudes_count, reposts_count, comments_count])
                    print([mid, text, place, t, attitudes_count, reposts_count, comments_count])

if __name__ == '__main__':
    Demo().start()  # Instantiate Demo and start crawling data